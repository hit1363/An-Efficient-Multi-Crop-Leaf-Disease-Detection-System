{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d45f45e",
   "metadata": {},
   "source": [
    "# Data Exploration - Multi-Crop Leaf Disease Dataset\n",
    "\n",
    "This notebook explores the multi-crop leaf disease dataset used for training the detection model.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Total Images: ~50,000\n",
    "- Crops: Tomato, Potato, Corn, Rice, Wheat\n",
    "- Classes: 30+ disease categories + healthy leaves\n",
    "- Split: 70% Train, 15% Val, 15% Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cfe85",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03950d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686c643",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_ROOT = \"../dataset\"\n",
    "RAW_DATA_PATH = os.path.join(DATASET_ROOT, \"raw\")\n",
    "PROCESSED_DATA_PATH = os.path.join(DATASET_ROOT, \"processed\")\n",
    "\n",
    "TRAIN_DIR = os.path.join(PROCESSED_DATA_PATH, \"train\")\n",
    "VAL_DIR = os.path.join(PROCESSED_DATA_PATH, \"val\")\n",
    "TEST_DIR = os.path.join(PROCESSED_DATA_PATH, \"test\")\n",
    "\n",
    "print(f\"Dataset root: {DATASET_ROOT}\")\n",
    "print(f\"Training data: {TRAIN_DIR}\")\n",
    "print(f\"Validation data: {VAL_DIR}\")\n",
    "print(f\"Test data: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b56f0",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics\n",
    "\n",
    "Let's analyze the dataset structure and compute basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_per_class(data_dir):\n",
    "    \"\"\"Count images in each class directory\"\"\"\n",
    "    class_counts = {}\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ö†Ô∏è Directory not found: {data_dir}\")\n",
    "        return class_counts\n",
    "    \n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            image_files = [f for f in os.listdir(class_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            class_counts[class_name] = len(image_files)\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Count images in each split\n",
    "train_counts = count_images_per_class(TRAIN_DIR)\n",
    "val_counts = count_images_per_class(VAL_DIR)\n",
    "test_counts = count_images_per_class(TEST_DIR)\n",
    "\n",
    "print(f\"Training classes: {len(train_counts)}\")\n",
    "print(f\"Validation classes: {len(val_counts)}\")\n",
    "print(f\"Test classes: {len(test_counts)}\")\n",
    "print(f\"\\nTotal training images: {sum(train_counts.values())}\")\n",
    "print(f\"Total validation images: {sum(val_counts.values())}\")\n",
    "print(f\"Total test images: {sum(test_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dffd74",
   "metadata": {},
   "source": [
    "## 4. Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "if train_counts:\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    classes = list(train_counts.keys())\n",
    "    counts = list(train_counts.values())\n",
    "    \n",
    "    plt.bar(range(len(classes)), counts, color='steelblue', alpha=0.7)\n",
    "    plt.xlabel('Disease Class', fontsize=12)\n",
    "    plt.ylabel('Number of Images', fontsize=12)\n",
    "    plt.title('Training Dataset - Class Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(classes)), classes, rotation=90, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Most common class: {max(train_counts, key=train_counts.get)} ({max(train_counts.values())} images)\")\n",
    "    print(f\"Least common class: {min(train_counts, key=train_counts.get)} ({min(train_counts.values())} images)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training data found. Please prepare the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9844b78",
   "metadata": {},
   "source": [
    "## 5. Sample Image Visualization\n",
    "\n",
    "Display sample images from different disease classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66875ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(data_dir, num_classes=6, images_per_class=3):\n",
    "    \"\"\"Display sample images from multiple classes\"\"\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ö†Ô∏è Directory not found: {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    classes = sorted(classes)[:num_classes]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_classes, images_per_class, figsize=(12, num_classes*2))\n",
    "    fig.suptitle('Sample Images from Different Disease Classes', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for j in range(images_per_class):\n",
    "            if j < len(images):\n",
    "                img_path = os.path.join(class_dir, images[j])\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "                if j == 0:\n",
    "                    axes[i, j].set_title(f\"{class_name}\", fontsize=10, fontweight='bold', loc='left')\n",
    "            else:\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display samples\n",
    "display_sample_images(TRAIN_DIR, num_classes=6, images_per_class=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51f635",
   "metadata": {},
   "source": [
    "## 6. Image Size Analysis\n",
    "\n",
    "Analyze the distribution of image sizes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_sizes(data_dir, sample_size=500):\n",
    "    \"\"\"Analyze image dimensions in dataset\"\"\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ö†Ô∏è Directory not found: {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspects = []\n",
    "    \n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    count = 0\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for img_name in images[:sample_size // len(classes)]:\n",
    "            try:\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                img = Image.open(img_path)\n",
    "                w, h = img.size\n",
    "                widths.append(w)\n",
    "                heights.append(h)\n",
    "                aspects.append(w/h)\n",
    "                count += 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"Analyzed {count} images\")\n",
    "    print(f\"\\nWidth  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.1f}\")\n",
    "    print(f\"Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.1f}\")\n",
    "    print(f\"Aspect Ratio - Min: {min(aspects):.2f}, Max: {max(aspects):.2f}, Mean: {np.mean(aspects):.2f}\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].hist(widths, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Width (pixels)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Image Width Distribution')\n",
    "    axes[0].axvline(np.mean(widths), color='red', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].hist(heights, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Height (pixels)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Image Height Distribution')\n",
    "    axes[1].axvline(np.mean(heights), color='red', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].hist(aspects, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].set_title('Aspect Ratio Distribution')\n",
    "    axes[2].axvline(np.mean(aspects), color='red', linestyle='--', label=f'Mean: {np.mean(aspects):.2f}')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_image_sizes(TRAIN_DIR, sample_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3ba11",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks\n",
    "\n",
    "Identify potential issues in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(data_dir):\n",
    "    \"\"\"Check for common data quality issues\"\"\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ö†Ô∏è Directory not found: {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    issues = {\n",
    "        'corrupted': [],\n",
    "        'too_small': [],\n",
    "        'grayscale': []\n",
    "    }\n",
    "    \n",
    "    MIN_SIZE = 50  # minimum acceptable dimension\n",
    "    \n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    total_checked = 0\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for img_name in images[:100]:  # Check first 100 per class\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                w, h = img.size\n",
    "                \n",
    "                # Check size\n",
    "                if w < MIN_SIZE or h < MIN_SIZE:\n",
    "                    issues['too_small'].append(img_path)\n",
    "                \n",
    "                # Check if grayscale\n",
    "                if img.mode == 'L':\n",
    "                    issues['grayscale'].append(img_path)\n",
    "                \n",
    "                total_checked += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                issues['corrupted'].append(img_path)\n",
    "    \n",
    "    print(f\"‚úÖ Data Quality Report\")\n",
    "    print(f\"Total images checked: {total_checked}\")\n",
    "    print(f\"\\nüìä Issues Found:\")\n",
    "    print(f\"  - Corrupted files: {len(issues['corrupted'])}\")\n",
    "    print(f\"  - Too small (<{MIN_SIZE}px): {len(issues['too_small'])}\")\n",
    "    print(f\"  - Grayscale images: {len(issues['grayscale'])}\")\n",
    "    \n",
    "    if sum(len(v) for v in issues.values()) == 0:\n",
    "        print(\"\\n‚ú® No issues found!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Review and fix identified issues before training\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "quality_issues = check_data_quality(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81394dc",
   "metadata": {},
   "source": [
    "## 8. Summary & Recommendations\n",
    "\n",
    "Based on the dataset analysis, here are the key findings and recommendations for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASET SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if train_counts:\n",
    "    total_images = sum(train_counts.values())\n",
    "    num_classes = len(train_counts)\n",
    "    avg_per_class = total_images / num_classes\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total classes: {num_classes}\")\n",
    "    print(f\"  ‚Ä¢ Total training images: {total_images}\")\n",
    "    print(f\"  ‚Ä¢ Average per class: {avg_per_class:.0f}\")\n",
    "    print(f\"  ‚Ä¢ Class imbalance ratio: {max(train_counts.values())/min(train_counts.values()):.2f}:1\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Recommendations:\")\n",
    "    \n",
    "    # Check class balance\n",
    "    imbalance = max(train_counts.values()) / min(train_counts.values())\n",
    "    if imbalance > 2:\n",
    "        print(f\"  1. Apply class weights during training (imbalance: {imbalance:.1f}:1)\")\n",
    "    else:\n",
    "        print(f\"  1. Dataset is relatively balanced ‚úì\")\n",
    "    \n",
    "    # Check dataset size\n",
    "    if avg_per_class < 500:\n",
    "        print(f\"  2. Consider data augmentation (avg {avg_per_class:.0f} images/class)\")\n",
    "    else:\n",
    "        print(f\"  2. Dataset size is adequate ‚úì\")\n",
    "    \n",
    "    print(f\"  3. Use transfer learning with ImageNet weights\")\n",
    "    print(f\"  4. Target input size: 224x224 pixels\")\n",
    "    print(f\"  5. Apply standard augmentations (rotation, flip, zoom)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No dataset found!\")\n",
    "    print(\"\\nTo prepare the dataset:\")\n",
    "    print(\"  1. Download PlantVillage dataset\")\n",
    "    print(\"  2. Organize into train/val/test splits\")\n",
    "    print(\"  3. Run preprocessing script\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
